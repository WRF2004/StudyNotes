## 概念复习

### 第1章和第2章

#### 操作系统定义：

* 操作系统是一组管理计算机硬件资源的软件集合，它向计算机程序提供共性的服务。



#### 操作系统的工作模式

* 内核态/管态 
* 用户态/目态



#### 并发与并行

* 并发：两个或多个事件在**同一时间间隔**内发生
* 并行：两个或多个事件在**同一时刻**发生



#### 批处理

* 批处理技术：同时输入多个作业，作业调度程序自动选择作业运行。
* 联机批处理系统：作业的输入输出和运行都由CPU完成
* 脱机批处理系统：作业的输入输出由输入机、输出机完成，运行由CPU完成，加入磁带做中间传递物
* 缺点：缺少交互性



#### **分时操作系统**：

* 分时是指**多个用户分享**使用同一台计算机。多个程序分时共享硬件和软件资源。
* 可实现人机交互的功能



#### **多道程序设计**：

* 概念：指允许多个程序同时装入内存，并允许它们交替在CPU中运行，共享系统中的各种硬、软件资源。当一道 程序因I/O请求而暂停运行时，CPU便立即转去运 行另一道程序。
* 即多道批处理系统的I/O设备可与CPU**并行**工作，这是借助**中断技术**实现的。
* 优点：
  * 资源利用率高
  * 系统吞吐量大
* 缺点：
  * 用户响应时间长
  * 不提供人机交互能力



#### **操作系统的功能**：

* 处理机管理
  * 进程（线程）控制
  * 进程（线程）同步
  * 进程通信
  * 进程（线程）调度。
* 存储器管理
  * 内存分配：静态和动态分配
  * 内存保护
  * 地址映射
  * 内存扩充。
* 文件管理
  * 文件存储空间的管理
  * 目录管理
  * 文件读、写管理
  * 文件保护
  * 向用户提供接口。
* 设备管理
  * 缓冲管理
  * 设备分配
  * 设备处理
  * 虚拟设备功能。



#### **现代操作系统的基本特征**：

* 并发执行
* 资源共享(复用)
* 虚拟化管理
* 异步性，不确定性事件的处理。



#### 冯诺依曼体系结构

* 存储程序式，集中顺序过程控制



#### **特权指令**：

* 不允许用户直接使用的指令



#### 异常与中断

* 中断：来自CPU执行指令外部的事件，主要由I/O设备、处理器时钟或定时器等硬件产生，可以被启用或禁用。
* 异常：来自CPU执行指令内部的事件，在相同条件下，异常可以重现。
  * 故障：指令执行引起的异常，如非法指令、缺页故障、除数为0等
  * 陷阱：用户进程中某一特定指令执行的结果，”事先安排的“，如系统调用
  * 终止：硬件故障



#### 系统调用

* 提供操作系统服务的编程接口
* 与函数调用的区别
  * 用户态到内核态，切换堆栈 
  * 移植性差 
  * 开销较大
* 过程：先跳转到异常分发代码，通过异常类型调用handle_sys函数根据系统调用号完成系统调用的实现。



#### 操作系统内核

* 是一个操作系统的核心
* 负责管理系统的进程、内存、设备驱动程序、文件和网络系统，决定着系统的性能和稳定性。
* 通过异常来陷入内核态。



#### 微内核

* 内核只完成不得不完成的功能，其他诸如文件系统、内存管理、设备驱动等的内容都被作为系统进程放到了用户态空间。
* 好处：架构独立，减小系统耦合，增加可移植性。
* 坏处：频繁系统调用效率难以保证。



#### MBR(主引导记录)

* 硬盘上第0磁头第0磁道第一个扇区被称为MBR，即主引导记录，它的大小是512字节，里面存放了预启动信息 、分区表信息。
* 位于磁盘的固定 位置(sector 1 of cylinder 0, head 0)



### 第3章

#### 存储管理目标与功能

* 存储管理目标

  * 地址独立：程序发出的地址与物理地址无关

  * 地址保护：一个程序不能访问另一个程序的地址空间。

* 存储管理要解决的问题：分配和回收。

* 存储管理的功能：

  * 内存的分配与回收
  * 地址转换(动静态重定位)
  * 存储共享与保护
  * 内存容量扩充

#### 程序的链接与装入

* 编译：将源代码编译成若干目标模块
* 链接：将编译后形成的一组目标模块以及它们所需的库函数链接在一起，形成一个完整的装入模块(可执行文件)
* 装入：将装入模块装入内存运行

#### 进程的内存映像

* .bss: 未初始化以及所有初始化为0的全局变量和静态变量
* .data: 已初始化的全局变量和静态变量
* .text: 用户程序的执行指令
* .rodata: 只读数据
* .init: 程序初始化时调用的_init函数
* 栈：用户存放程序临时创建的局部变量，也就是说我们函数括弧“{}”中定义的变量（但不包括static声明的变量，static意味着在data段中存放变量）。

#### 地址空间：

* 一个进程所能够用于访问内存的地址集合。

#### 逻辑地址：

* 又称虚拟地址，程序所使用的地址。

#### 物理地址：

* 物理内存中数据存储的地址



#### 分区存储管理

* 把内存分为一些大小相等或不等的分区
* 适用于多道程序系统和分时系统，支持多个程序并发执行，但难以进行内存分区的共享。

##### **碎片**

* 内存中无法被利用的存储空间。

* 外碎片：

  * 分区与分区之间的碎片，可以用**紧凑技术**消除外碎片。

  * 外部碎片才是造成内存系统性能下降的主要原因。外部碎片可以被整理后清除

* 内碎片：指分配给作业的存储空间中未被利用的部分

* **紧凑技术**：

  * 通过移动作业，把多个分散的小分区拼接成一个大分区的方法称为紧凑（拼接或紧缩）。
  * 目标：消除外部碎片，使本来分散的多个小空闲分区连成一个大的空闲区。
  * 紧凑时机：找不到足够大的空闲分区且总空闲分区容量可以满足作业要求时。

##### 固定式分区

* 把内存划分为若干个**固定大小**的连续分区，当系统初始化时，把存储空间划分成若干个任意大小的区域；然后，把这些区域分配给每个用户作业。
  * 分区大小相等：只适合于多个相同程序的并发执行（处理多个类型相同的对象）。
  * 分区大小不等：多个小分区、适量的中等分区、少量的大分区。根据程序的大小，分配当前空闲的、适当大小的分区。
* 优点：易于实现，开销小。
* 缺点：内碎片造成浪费，分区总数固定，限制了并发执行的程序数目。
* 分配方式：
  * **单一队列**的分配方式：当需要加载程序时，选择一个当前闲置且容量足够大 的分区进行加载，可采用共享队列的固定分区（**多个用户程序排在一个共同的队列里面等待分区**）分配。
  * **多队列**分配方式：由于程序大小和分区大小不一定匹配，有可能形成一 个小程序占用一个大分区的情况，从而造成内存里虽 然有小分区闲置但无法加载大程序的情况。这时，可以采用多个队列，给每个分区一个队列，程序按照所需内存的大小排在相应的队列里。

##### 可变式分区

* 分区的边界可以移动，即分区的大小可变。
* 优点：没有内碎片。
* 缺点：有外碎片。

##### 闲置空间的管理

* 在管理内存时，OS需要知道内存空间有多少空闲。这 就必须跟踪内存的使用，跟踪的办法有两种：位图表示法（分区表）和链表表示法（分区链表）

* 位图表示法：

  * 给每个分配单元赋予一个二进制数位，用来记录该分配单元是否闲置。
  * 空间开销固定：不依赖于内存中的程序数量。
  * 时间开销低：操作简单，直接修改其位图值即可。
  * 没有容错能力：如果一个分配单元对应的标志位为1，不能确定是否因错 误变成1。

* 链表表示法：

  * 将分配单元按照是否闲置链接起来，这种方法称为链表表示法。
  * 由“标志位(空闲或程序)+起始地址+空间长度+链表指针”构成。
  * 空间开销：取决于程序的数量。
  * 时间开销：链表扫描速度较慢，还要进行链表项的插入、删除和修改。
  * 有一定容错能力：因为链表有被占空间和闲置空间的表项，可相互验证。

  ![微信截图_20240531223037](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240531223037.png)

##### 基于顺序搜索的分配算法

* 首次适应算法（First Fit）：
  * 空闲分区按地址递增的次序排列，从这个空白区域链的始端开始查找，选择第一个足以满足请求的空白块。
  * 低地址部分留下了很多很小空闲空间，增加了查找开销。
* 下次适应算法（Next Fit）：

  * 每次为存储请求查找合适的分区时，总是从上次分配的下一块开始，只要找到一个足够大的空白区，就将它划分后分配出去。
  * 使小空闲分区均匀分布，但会导致缺乏大空闲分区。

* 最佳适应算法（Best Fit）：

  * 将空闲分区按容量递增排列, 总是寻找其大小最接近于作业所要求的存储区域。

  * 留下许多难以利用的小空闲区（碎片）。

* 最坏适应算法（Worst Fit）：

  * 将空闲分区按容量递减排列, 总是寻找最大的空白区。

  * 当大作业来时，可能没有那么大的空间了。

##### 基于索引搜索的分配算法

* 快速适应算法
  * 把空闲分区按容 量大小进行分类，常用大小的空闲区设立单独的空闲 区链表。系统为多个空闲链表设立一张管理索引表。
  * 优点：
    * 查找效率高，仅需要根据程序的长度，寻找到能容纳它的最小空闲区链表 ，取下第一块进行分配即可。 
    * 该算法在分配时，不会对任何分区产生分割，所以能保留大的分区，也不 会产生内存碎片。
  * 缺点：
    * 分区回收算法复杂（链表的插入、分区合并等），系统开销较大。在分配空闲分区时是以进程为单位，一个分区只属于一个进程，存在一定的浪费 。（空间换时间）
* **伙伴系统**
  * 伙伴系统(buddy system)是介于固定分区与可变分区之间的动态分区技术。
  * **伙伴：在分配存储块时将一个大的存储块分裂成两个大小相等的小块，这两个小块就称为“伙伴”。**
  * 伙伴系统规定，无论已分配分区或空闲分区，其大小均为2的k次幂，k为整数，n≤k≤m，其中:2的n次方表示分配的最小分区的大小，2的m次方表示分配的最大分区的大小，通常 2的m次方是整个可分配内存的大小。
  * 在系统运行过程中，由于不断的划分，可能会形成若干个不连续的空闲分区。
  * 内存管理模块保持有多个空闲块链表，空闲块的大小可以为1，2，4，8，...，2的m次方字节
  * **伙伴系统的内存分配**
    * 系统初启时，只有一个最大的空闲块(整个内存)。当一个长度为n的进程申请内存时，系统就分给它一个大于或等于所申请尺寸的最小的 2 的幂次的空闲块。如果 2的i-1次方<n≤2的i次方，则在空闲分区大小为 2的i次方的空闲分区链表中查找。
    * 若找到大小为2的i次方的空闲分区，即把该空闲分区分配给进程。否则表明长度为2的i次方的空闲分区已经耗尽，则在分区大小为 2的i+1次方的空闲分区链表中寻找。
    * 若存在 2的i+1次方的一个空闲分区，把该空闲分区分为相等的两个分区，这两个分区称为一对伙伴，其中的一个分区用于分配，另一个加入大小为 2的i次方的空闲分区链表中。若大小为 2的i+1次方的空闲分区也不存在，需要查找大小为2的i+2次方的空闲分区，若找到则对其进行两次分割:第一次，将其分割为大小为 2的i+1次方的两个分区，一个用于分割，一个加入到大小为 2的i+1次方的空闲分区链表中;第二次，将用于分割的空闲区分割为2的i次方两个分区，一个用于分配，一个加入到大小为 2的i次方的空闲分区链表中。若仍然找不到，则继续查找大小为 2的i+3次方 的空闲分区，以此类推。
  * **伙伴系统的内存释放**
    * 首先考虑将被释放块与其伙伴合并成一个大的空闲块然后继续合并下去，直到不能合并为止。
    * 如果有两个存储块大小相同，地址也相邻，但不是由同一个大块分裂出来的(不是伙伴）则不会被合并起来。
  * **伙伴系统特点**
    * 优点：伙伴系统利用计算机二进制数寻址的优点，加速了相邻空闲分区的合并。
    * 缺点：不能有效地利用内存。进程的大小不一定是 2 的整数倍，由此会造成浪费，内部碎片严重。

##### 分区的存储保护

* 存储保护是为了防止一个作业有意或无意地破坏操作系统或其它作业。
* 常用的存储保护方法:
  * 界限寄存器方法：
    * 上下界寄存器方法
    * 基址、限长寄存器(BR,LR) 方法
  * 存储保护键方法：
    * 给每个存储块分配一个单独的保护键，它相当于一把锁
    * 进入系统的每个作业也赋予一个保护键，它相当于一把钥匙
    * 当作业运行时，检查钥匙和锁是否匹配，如果不匹配，则系统发出保护性 中断信号，停止作业运行

##### 覆盖与交换

* 覆盖与交换技术是在多道程序环境下用来扩充内存的两种方法。
* 覆盖与交换可以解决在小的内存空间运行大作业的问题，是“扩充”内存容量和提高内存利用率的有效措施。
* 覆盖技术主要用在早期的OS 中，交换技术则用在现代OS中。
* **覆盖**
  * 把一个程序划分为一系列功能相对独立的程序段，让执行时不要求同时装入内存的程序段组成一组（称为覆盖段），共享主存的同一个区域，这种内存扩充技术就是覆盖。
  * 缺点：对用户不透明，增加了用户负担。
* **交换**
  * 把暂时不用的某个（或某些）程序及其数据的部分或全部从主存移到辅存中去，以便腾出存储空间；接着把指定程序或数据从辅存读到相应的主存中，并将控制转给它，让其在系统上运行。
  * 优点：
    * 增加并发运行的程序数目，并且给用户提供适当的响应时间
    * 编写程序时不影响程序结构
  * 缺点：
    * 对换入和换出的控制增加处理机开销
    * 程序整个地址空间都进行传送，没有考虑执行过程中地址访问的统计特性。
* **覆盖与交换技术的区别**
  * 覆盖可减少一个程序运行所需的空间。交换可让整个程序暂存于外存中，让出内存空间。
  * 覆盖是由程序员实现的，操作系统根据程序员提供的覆盖结构来完成程序段之间的覆盖。交换技术不要求程序员给出程序段之间的覆盖结构。
  * 覆盖技术主要对同一个作业或程序进行。交换主要在作业或程序间之间进行。



#### 页式存储管理

* 基本思想：把一个逻辑地址连续的程序分散存放到若干不连续的内存区域内，并保证程序的正确执行。
* 纯分页系统：不具备页面对换功能，不支持虚拟存储器功能

##### 程序、进程和作业

* 程序是静止的，是存放在磁盘上的可执行文件
* 进程是动态的。
  * 进程包括程序和程序处理对象（ 数据集），是一个程序的执行过程，是分配资源的基本单位。
  * 通常把进程分为系统进程和用户进程：
    * 系统进程：完成操作系统功能的进程
    * 用户进程：完成用户功能的进程
* 作业是用户需要计算机完成的某项任务，是要求计算机所做工作的集合。
* 作业、进程和程序之间的联系
  * 作业通常包括程序、数据和操作说明书3部分
  * 每一个**进程由进程控制块PCB、程序和数据集合组成**。这说明程序是进程的一部分，是进程的实体
  * 一个作业可划分为若干个进程来完成，而每一个进程有其实体——程序和数据集合。

##### 页(页面)

* 在分页存储管理系统中，把每个作业的地址空间分成一些大小相等的片，称之为页面或页，各页从0开始编号。

##### 存储块(页框)

* 把物理内存的存储空间也分成与页面相同大小的片，这些片称为存储块，或称为页框（Frame） ，同样从0开始编号。

##### 页面的大小

* 若页面较小：
  * 减少页内碎片和总的内存碎片，有利于提高内存利用率。
  * 每个进程页面数增多，使页表长度增加，占用内存较大。
* 若页面较大：
  * 每个进程页面数减少，页表长度减少，占用内存较小
  * 增加页内碎片增大，不利于提高内存利用率。

##### 页表

* 为便于在内存找到进程的每个页面所对应的页框，分页系统中为每个进程配置一张页表。进程逻辑地址空间的每一页，在页表中都对应有一个页表项。
* 地址变换是借助页表实现的
* 页表存放在内存中，属于进程的现场信息。
* 访问一个数据需访问内存2 次(页表一次，内存 一次)。
* 页表的基址及长度由页表寄存器给出。
* 地址变换机构(MMU)：
  * 当进程要访问某个逻辑地址中的数据时，分页地址变换机构会自动地将有效地址（相对地址）分为“页号 ”和“页内地址”两部分。
  * 将“页号”与“页表长度”进行比较，如果页号大于或等于页表长度，则表示本次所访问的地址已超越进程的地址空间，产生地址越界中断。
  * 将“页表始址”与“页号和页表项长度的乘积”相加 ，得到该页表项在页表中的位置，于是可从页表项中得到该页的物理块号，将之装入物理地址寄存器中。
  * 将有效地址寄存器中的“页内地址”送入物理地址寄存器的“块内地址”字段中（块内地址= 页内地址）。

##### 逻辑地址

* 如果逻辑地址空间为2 ^ m，且页面大小为2 ^ n单元，那么逻辑地址的高m-n位表示页号 （页表的索引），而低n位表示页偏移。
* 假设逻辑地址为A，页面大小为L，则页号P = A / L, 页内偏移W = A % L

##### 物理地址

* 如果物理地址空间为2 ^ m，且页框大小为2 ^ n单元，那么物理地址的高m-n位表示块号，而低n位表示块内偏移。

##### 二级页表

* 将页表再进行分页，离散地将各个页表页面存放在不同的物理块中，同时也再建立一个外层页表用以记录 页表页面对应的物理块号。
* 正在运行的进程先把外层页表（页表的页表）调入内存，而后动态调入内层页表。只将当前所需的一些内层页表装入内存，其余部分根据需要再陆续调入。

##### 多级页表

* 多级页表结构中，指令所给出的地址除偏移地址之外的**各部分全是各级页表的页表号或页号**，而**各级页表中记录的全是物理页号**，指向下级页表或真正的被访问页。
* 多级页表解决了当逻辑地址空间过大时，页表长度大大增加的问题。
* 多级页表带来的问题：大大增加了一次访存的时间。

##### 具有快表的地址变换机构(TLB)

* 快表又称TLB  (Translation Lookaside Buffer) 转换表查找缓冲区
* 快表是一种特殊的高速缓冲存储器（Cache），内容 是页表中的一部分或全部内容。
* CPU 产生逻辑地址的页号，首先在快表中寻找，若命中就找出其对应的物理块；若未命中，再到页表中找其对应的物理块，**并将相应的页表项复制到快表**。若快表中内容满，则按某种算法淘汰某些页。
* 有效内存访问时间（一级页表）：
  * EAT-Effective Access Time
  * TLB查询时间= a 时间单位
  * 单次内存访问时间= b 时间单位
  * TLB的命中率是 α
  * EAT的计算：EAT = (a + b) * α + (a + 2 * b) * (1 - α) = 2b + a -bα

##### 哈希页表

* 处理超过32位地址空间的一种常用方法是使用哈希页表（hashed page table）

* 根据虚拟页号的哈希值来访问页表。

* 哈希页表的每一表项都包括一个链表的元素，这些元素哈希成同一位置（要处理碰撞）。每 个元素有3个域：

  * 虚拟页号
  * 所映射的页框号
  * 指向链表中下一个元素的指针。

* 具体过程：

  * 将虚拟页号转换为哈希值，据此访问哈希表的表项(链表)。
  * 用虚拟页号与链表中的元素的第一个域相比较。
    * 如果匹配，那么相应的页框号 （第二个域）就用来形成物理地址；
    * 如果不匹配， 那么就进一步比较链表的下一个节点，以找到匹配

  ![微信截图_20240601173340](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240601173340.png)

##### 反置页表

* 反置页表不是依据进程的逻辑页号来组织，而是依据该进程在内存中的页框号来组织（即：按页框号排列），其表项的内容是逻辑页号P及隶属进程标志符pid。

* 反置页表的大小只与物理内存的大小相关，与逻辑空间大小和进程数无关。如: 64M主存,若页面大小为 4K,则反向页表只需64KB。

* 利用反置页表进行地址变换：

  * 用进程标志符和页号去检索反置页表。
  * 如果检索完整个页表未找到与之匹配的页表项，表明此页此时尚未调入内存，对于具有请求调页功能的存储器系统产生请求调页中断，若无此功能则表示地址出错。
  * 如果检索到与之匹配的表项，则表项的序号i 便是该页的物理块号，将该块号与页内地址一起构成物理地址。
  * 采用反置页表的系统很难共享内存，因为每个物理帧只对应一个虚拟页条目。

  ![微信截图_20240601173904](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240601173904.png)

##### 页共享与保护

* 页的保护
  * 地址越界保护
  * 在页表中设置保护位（定义操作权限：只读，读写，执行等）
* 共享带来的问题
  * 若共享数据与不共享数据划在同一页框中，则：有些不共享的数据也被共享，不易保密。
  * 实现数据共享的最好方法：分段存储管理。

##### 页目录自映射

* 自映射：页目录中有一条页表项指向自身物理地址，也就是页目录基地址
* 构建方法：
  * 给定一个页表基址PTbase ，该基址需4M对齐，即： PTbase  =（（ PTbase）>> 22）<< 22；
  * 页目录表基址PDbase= PTbase |(PTbase)>>10
  * 自映射目录表项PDEself-mapping  = PTbase |(PTbase)>>10| (PTbase)>>20
  * “页目录自映射”的含义是页目录包含在页表当中 ，是我们采用的映射（或组织）方法的一个特征， **是虚拟地址空间内的映射，与虚拟地址到物理地址 的映射无关！**
  * 举例：
    * 给定页表虚拟地址起始位置，例如0x7fc00000 
    * 一个页表可映射4MB(1K * 4KB)空间, 所以上述地址对应于第（0x7fc00000>>22）个页表
    * 又因为一个页表占用4KB空间，所以（0x7fc00000>>22）<<12表示该页表对应页表始址的偏移。
    * 又因为一个页表中有1K个页表项，所以((0x7fc00000>>22)<<12)>>10)表示页表项相对于页表的偏移。



#### 段式存储管理

##### 段式存储管理的主要动机

* 方便编程：
  * 通常一个作业是由多个程序段和数据段组成的，**用户一般按逻辑关系对作业分段，并能根据名字来访问程序段和数据段。**
* 信息共享：
  * 共享是以信息的逻辑单位为基础的。**页是存储信息的物理单位，段是信息的逻辑单位。**
  * 页式管理中地址空间是一维的，主程序、子程序都顺序排列，共享公用子程序比较困难，一个共享过程可能需要几十个页面。
* 信息保护：
  * 页式管理中，一个页面中可能装有2个不同的子程序段的指令代码，不能通过页面共享实现一个逻辑上完整的子程序或数据块的共享。
  * 段式管理中，可以使用信息的逻辑单位进行保护。
* 动态增长
  * 实际应用中，某些段（数据段）会不断增长，前面的存储管理方法均难以实现。
* 动态链接：
  * 动态链接在程序运行时才把主程序和要用到的目标程序 （程序段）链接起来。

##### 分段

* 分段系统将用户进程的逻辑地址空间划分为大小不等的段。
* 每段都有自己的名字（通常是段号），每段都是从0开始编址，并分配一段连续的地址空间。

##### 段表

* 段表记录了段与内存位置的对应关系。
* 段表保存在内存中。
* 段表的基址及长度由段表寄存器给出。
* 每个进程都有一张逻辑空间与内存空间映射的段表
* 进程的每个段对应一个段表项
* 段表项记录了该段在内存中的始址和段的长度
* 访问一个字节的数据/指令需访问内存两次(段表一 次，内存一次)。
* 逻辑地址由段和段内地址组成。

##### 地址变换

* 分段系统中设置了一个段表寄存器，存储了段表的基址及长度。

* 将逻辑地址中的段号S 与段表长度TL 进行比较。
  * 若S>TL，表示访问越界，产生越界中断。
  * 若未越界，则根据段表的始址和该段的段号，计算出该段对应段表项的位置，从中读出该段在内存的始址。
* 再检查段内地址d，是否超过该段的段长SL。
  * 若超过，即d >SL，同样 发出越界中断信号。
  * 若未越界，则将该段的基址与段内地址d 相加， 即可得到要访问的内存物 理地址。

##### 分页与分段的比较

* 相同之处：
  * 二者均是非连续分配方式，都要通过地址映射机构实现地址变换
* 不同之处：
  * 页是信息的物理单位，用户不可见
  * 段是信息的逻辑单位，用户可见
  * 页的大小固定且由系统决定。
  * 段的长度不固定，取决于用户编写的程序。
  * 分页管理的地址空间是一维的。
  * 分段管理的地址空间是二维的，因为每段长度不固定，无法通过除法得出段号，无法通过求余得出段内偏移。

![微信截图_20240602143355](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240602143355.png)

##### 可重入代码

* 不能被任何进程修改的代码称为可重入代码或者纯代码（不属于临界资源）

##### 分段管理的优缺点

* 优点：
  * 更好地实现数据共享与保护;段长可动态增长
  * 便于动态链接
* 缺点：
  * 处理机要为地址变换花费时间；要为段表提供附加的存储空间。
  * 为满足分段的动态增长和减少外碎片，要采用内存紧凑的技术手段。 
  * 在辅存中管理不定长度的分段比较困难（交换）。 
  * 分段的最大尺寸受到主存可用空间的限制。

#### 段页式存储管理

* 基本思想：用分段方法来分配和管理虚拟存储器，而用分页方法来分配和管理实存储器。
* 分段和分页原理的结合，即先将用户程序分成若干个段（段式），并为每一个段赋一个段名， 再把每个段分成若干个页（页式）。
* 逻辑地址的地址结构：
  * **段号、段内页号、页内位移三部分**所组成
  * 系统中设段表和页表，均存放于内存中。读一次指令或数据须访问内存三次。为提高速度可增设高速缓存。
  * 每个进程一张段表，每个段一张页表。
  * **段表含段号、页表始址和页表长度**；页表包含页号和页框号。
* 例子：x86的段页式地址映射
  * 段映射机制，将逻辑地址映射到线性地址；
  * 页映射机制，将线性地址映射到物理地址。

#### 虚拟内存管理

##### 局部性原理

* 时间局部性
  * 即一条指令的一次执行和下次执行，一个数据的一次访问和下次访问都集中在一段较短时间内
  * 即当前指令和邻近的几条指令，当前访问的数据和邻近的数据都集中在一个较小区域内

##### 虚拟内存

* 给所有进程提供**一致的地址空间**，每个进程都认为自己是在独占使用单机系统的存储资源
* **保护每个进程的地址空间**不被其他进程破坏，隔离了进程的地址访问
* 根据缓存原理，上层存储是下层存储的缓存，**虚拟内存把主存作为磁盘的高速缓存**，在主存和磁盘之间根据需要来回传送数据，高效地使用了主存

##### 虚拟存储的基本原理

* 在程序装入时，不需要将其全部读入到内存，而只需将当前需要执行的部分页或段读入到内存，就可让程序开始执行。
* 在程序执行中，如果需执行的指令或访问的数据尚未在内存（称为缺页或缺段），则由处理器通知操作系统将相应的页或段调入到内存，然后继续执行程序 **（请求调入功能）**
* 操作系统将内存中暂时不使用的页或段调出保存在外存上，从而腾出空间存放将要装入的程序以及将要调 入的页或段。**（置换功能）**

##### 虚拟存储技术的特征

* 离散性
  * 物理内存分配的不连续，虚拟地址空间使用的不连续（数据段和栈段之间的空闲空间，共享段和动态链接库占用的空间）
* 多次性
  * 作业被分成多次调入内存运行。正是由于多次性 ，虚拟存储器才具备了逻辑上扩大内存的功能。
  * 多次性是虚拟存储器最重要的特征，其它任何存储器不具备这个特征。
* 对换性
  * 允许在作业运行过程中进行换进、换出。
  * 换进、 换出可提高内存利用率。
* 虚拟性
  * 允许程序从逻辑的角度访问存储器，而不考虑物理内存上可用的空间容量。 
  * **范围大，但占用容量不超过物理内存和外存交换区容量之和**。 
  * 占用容量包括：进程地址空间中的各个段，操作系统代码。
* 虚拟性以多次性和对换性为基础， 多次性和对换性必须以离散分配为基础。

##### 优点、代价和限制

* 优点
  * 可在较小的可用(物理)内存中执行较大的用户程序
  * 可在(物理)内存中容纳更多程序并发执行
  * 不必影响编程时的程序结构（与覆盖技术比较）
  * 提供给用户可用的虚拟内存空间通常大于物理内存

* 代价
  * 虚拟存储量的扩大是以牺牲CPU 处理时间以及内外存交换时间为代价。
* 限制
  * 虚拟内存的最大容量主要由计算机的地址结构决定 。例如: 32 位机器的虚拟存储器的最大容量就是4GB。

##### 与Cache-主存机制的异同

* 相同点
  * 出发点相同：二者都是为了提高存储系统的性能价格比而构造的分层存储体系，都力图使存储系统的性能接近高速存储器，而价格和容量接近低速存储器。 
  * 原理相同：都是利用了程序运行时的局部性原理把最近常用的信息块从相对慢速而大容量的存储器调入相对高 速而小容量的存储器
* 不同点
  * 侧重点不同：
    * cache主要解决主存与CPU的速度差异问题
    * 虚存主要解决存储容量问题，另外 还包括存储管理、主存分配和存储保护等。 
  * 数据通路不同
    * CPU与cache和主存之间均有直接访问通路，cache不命中时可直接访问主存 
    * 虚存所依赖的辅存与CPU之间不存在直接的数据通路，当主存不命中时只能通过调页解决，CPU最终还是要访问主存。
  * 透明性不同
    * cache的管理完全由硬件完成， 对系统程序员和应用程序员均透明
    * 虚存管理由软件（OS）和硬件共同完成，由于软件的介入，虚存对实现存储管理的系统程序员不透明，而只对应用程序员透明（段式和段页式管理对应用程序员“半透明”）
  * 未命中时的损失不同
    * 由于主存的存取时间是 cache的存取时间的5～10倍，而主存的存取速 度通常比辅存的存取速度快上千倍，故主存未命中时系统的性能损失要远大于cache未命中时的损失。

##### 请求分页（段）系统

* 在分页(段)系统的基础上，增加了请求调页(段)功能、页面(段)置换功能所形成的页(段)式虚拟存储器系统。
* 允许只装入若干页(段)的用户程序和数据，便可启动运行 ，以后在硬件支持下通过**调页(段)功能和置换页(段)功能** ，陆续将要运行的页面(段)调入内存，同时把暂不运行的页面(段)换到外存上，置换时以页面(段)为单位。

* 系统须提供相应的硬件和软件支持：
  * 硬件支持：请求分页(段)的页(段)表机制、缺页(段) 中断机构和地址变换机构。
  * 软件：请求调页(段)功能和页(段)置换功能的软件。
* 页表项组成：
  * 页号
  * 访问位：用于页面置换算法。
  * 修改位：表明此页在内存中是否被修改过。
  * 保护位：只读、可写、可执行。
  * 驻留位：1表示该页位于内存当中；0表示该页当前还在外存当中。
  * 物理块号

##### 页面调入策略

* 按需调页
  * 当且仅当需要某页时才将其调入内存的技术称为按需调页，被虚拟内存系统采用。
  * 按需调页系统类似于使用交换的分页系统，进程驻留在外存（磁盘），进程执行时使用懒惰交换（lazy swapper） 换入内存。一次装入请求的一个页面，磁盘I/O的启动频率较高，系统的开销较大
  * 按需调页需要使用外存，保存不在内存中的页，通常为快速磁盘，用于和内存交换页的部分空间称为交换空间
* 预调页
  * 当进程开始时，所有页都在磁盘上，每个页都需要通过页错误（Page Fault,也称缺页异常）来调入内存 。预调页同时将所需要的所有页一起调入内存，从而阻止了大量的页错误。部分操作系统如Solaris 对小文件就采取预调页调度。
  * 实际应用中，可以**为每个进程维护一个当前工作集合 （working set，简称工作集）**中的页的列表，如果进程在暂停之后需要重启时，根据这个列表使用预调 页将所有工作集合中的页一次性调入内存。
  * 若程序执行的局部性较差，则预先装入的很多页面不会很快被引用，并会占用大量的内存空间，反而降低系统的效率。

##### 缺页中断处理

* 陷入内核态，保存现场。
* 查找发生缺页的虚拟页面 
* 检查虚拟地址的有效性和保护位。
* 找一个空闲的页框(物理内存中的页面)，如果没有的话使用页面置换算法换出一个页框。
* 如果换出去的页框是dirty的，需要写回磁盘。
* 准备好页框后将所需页面从磁盘调入
* 向操作系统发出中断更新页表。
* 恢复现场继续执行。

##### 页面置换策略

* OPT：

  * 最优置换，无法被实现，但可以用于衡量其他页面置换算法的效果。

  * 从主存中移出永远不再需要的页面，如这样的页面不存在，则应选择未来最久不被使用的页面。

* FIFO：
  * 先进先出
  * 总选择作业中在主存驻留时间最长的一页淘汰
  * 新访问的页面插入FIFO队列尾部， 页面在FIFO队列中顺序移动
  * 淘汰FIFO队列头部的页面
  * 性能较差。较早调入的页往往是经常被访问的页，这些页在FIFO算法下被反复调入和调出。可能出现**Belady异常**现象。
  * **Belady异常现象**
    * 在使用FIFO算法作为缺页置换算法时，随着分配的页框增多，缺页率反而提高

* Second Chance

  * 改进的FIFO算法

  * 如果被淘汰的数据之前被访问过，则给其第二次机会

  * 每个页面都有一个标志位。用于标识此数据放入缓存队列后是否被再次访问过。删的时候如果没被访问过直接删，如果访问过的话清除标志然后跳过删下一个，这个下次再删。
  * A是FIFO队列中最旧的页面，且其放入队列后没有被再次访问，则A被立刻淘汰；否则，如果放入队列后被访问过，则将A移到FIFO队列尾部，并且将访问标志位清除
  * 如果所有页面都被访问过，则经过一次循环后就按FIFO原则淘汰。

* Clock

  * 改进的FIFO算法
  * 环形队列，没被访问过直接替换，被访问过的话清除标志删下一个。

  * 如果没有缺页错误，将所访问页的访问位置1，指针不动
  * 如果产生缺页错误
    * (1)如果当前页面的访问位是1：首先将当前页面的访问位置0，将指针向 前移一个位置；重复这个过程，直到找到访问位为0的页面，然后转(2)
    * (2)如果当前页面的访问位是0：则替换当前页面, 并将其访问位置为1， 并将指针向前移动一个位置。

![微信截图_20240602161656](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240602161656.png)

* LRU
  * 最近最久未使用
  * 每访问到一个已有的页，把它移到队尾。
  * 如果数据最近被访问过，那么将来被访问的几率也更高
  * LRU是局部性原理的合理近似，性能接近OPT算法 。但需记录页面使用的先后关系，实现开销大。

![微信截图_20240602163749](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240602163749.png)

##### 工作集

* 进程运行正在使用的页面集合
* 工作集的动态变化
  * 进程开始执行后，随着不断访问新页面逐步建立较稳定的工作集
  * 当内存访问的局部性区域的位置大致稳定时，工作集大小也大致稳定
  * 局部性区域的位置改变时，工作集快速扩张和收缩过渡到下一个稳定值。
* 工作集算法
  * 给定一个进程，记录其工作集 
  * 当需要进行页面替换时，选择不在工作集中的页面进行替换
* **引入工作集的目的是依据进程在过去的一段时间内访问的页面来调整驻留集大小。**

##### 驻留集

* 虚拟存储系统中， 每个进程驻留在内存的页面集合，或进程分到的物理页框集合
* 分配给每个活动进程的页框数越少，能够驻留内存的活动进程数就越多，进程调度程序能调度就绪进程的概率就越大， 然而，这将导致进程发生缺页中断的概率较大；
* 为进程分配过多的页框，并发运行的进程数降低，影响系统资源利用率。

##### 页面分配策略

* 固定分配策略
  * 为每个活动进程分配固定数量的页框。即每个进程的驻留集大小在运行期间固定不变。
* 可变分配策略
  * 为每个活动进程分配的页框数在其生命周期内是可变的。即系统首先给进程分配一定数量的页框，运行期间可以增/减页框。

##### 页面置换策略

* 局部置换策略
  * 系统在进程自身的驻留集中判断当前是否存在空闲页框，并在其中进行置换。
* 全局置换策略
  * 在整个内存空间内判断有无空闲页框 ，并允许从其它进程的驻留集中选择一个页面换出内存。

##### 抖动

* 王道：在页面置换过程中，刚刚换入的的页面马上又要被换出，刚刚换出的页面马上又要被换入，这种频繁的页面调度行为称为抖动。

* ppt：

  * 随着驻留内存的进程数目增加，即进程并发程度的提高，处理器利用率先上升，然后下降。

  * 每个进程的驻留集不断减小，当驻留集小于工作集后，缺页率急剧上升，频繁调页使得调页开销增大。

##### 抖动的消除与预防

* 局部置换策略
  * 如果一个进程出现抖动，它不能从另外的进程那里夺取内存块，从而不会引发其他进程出现抖动，使抖动局限于一个小的范围内。然而这种方法并未消除抖动的发生。（微观层面）
  * 引入工作集算法（微观）
  * 预留部分页面（微观或宏观）
  * 挂起若干进程：当出现CPU利用率、而磁盘I/O非常频繁的情况时，就可能因为多道程序度太高而造成抖动。为此，可挂起一个或几个进程，以便腾出内存空 间供抖动进程使用，从而消除抖动现象。（宏观）

##### 写时复制技术

* 两个进程共享同一块物理内存，每个页面都被标志成 了写时复制。共享的物理内存中每个页面都是只读的 。如果某个进程想改变某个页面时，就会与只读标记 冲突，而系统在检测出页面是写时复制的，则会在内存中复制一个页框，然后进行写操作。新复制的页框对执行写操作的进程是私有的，对其他共享写时复制页面的进程是不可见的。

  ![微信截图_20240602170154](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240602170154.png)

* 优点
  
  * Linux的fork()使用写时复制(copy-on-write)实现， 它可以推迟甚至免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。只有在需要写入的时候，数据才会复制 ，从而使各个进程都拥有各自的拷贝。也就是说，资源的复制只有在需要写入的时候才进行。

##### 内存映射文件

* 基本思想：进程通过一个系统调用（mmap）将一个文件（或部分）映射到其虚拟地址空间的一部分，访问这个文件就像访问内存中的一个大数组，而不是对文 件进行读写。
* 在多数实现中，在映射共享的页面时不会实际读入页面的内容，而是在访问页面时，页面才会被每次一页的读入，磁盘文件则被当作后备存储。
* 当进程退出或显式地解除文件映射时，所有被修改页面会写回文件。
* 采用内存映射方式，可以方便地让多个进程共享一个文件。![微信截图_20240602170716](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240602170716.png)

##### 存储保护

* 界限保护（上下界界限地址寄存器）：所有访问地址必须在上下界之间；
* 用户态与内核态
* 存取控制检查
* 环保护：处理器状态分为多个环(ring)，分别具有不同的存储访问特权级别(privilege)，通常是级别高的在内环， 编号小（如0环）级别最高；可访问同环或更低级别环的数据；可调用同环或更高级别环的服务

### 第4章

#### 进程与线程的基本概念

##### 并发与并行

- 并发：设有两个活动在同一时间，处在各自的起点和终点之间的某一处。
- 并行：如果考虑两个程序,它们在同一时间度量下同时运行在不同的处理机上，则称这两个程序是并行执行的。
- 并发可能是真并行，可能是伪并行。

##### 进程的定义

* 进程是程序的一次执行；
* 进程是可以和别的计算并发执行的计算；
* 进程可定义为一个数据结构，及能在其上进行操 作的一个程序；
* 进程是一个程序及其数据在处理机上顺序执行时 所发生的活动；
* 进程是程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位。

##### 进程的特征

* 动态性：进程是程序的一次执行过程，因创建而产生，因调度而执行，因无资源而暂停，因撤消而消亡；而程序是静态实体。 
* 并发性：多个进程实体同时存在于内存中，能在一段时间内同时运行。
* 独立性：在传统OS中，进程是独立运行的基本单位
* 异步性：也叫制约性，进程之间相互制约，进程以各自独立的不可预知的速度向前推进。 
* 结构特征：程序、数据、进程控制块PCB

##### 进程与程序的区别

* 进程 = 数据 + 程序 + PCB
* 进程是动态的，程序是静态的
* 进程是暂时的，程序的永久的
* 通过多次执行，一个程序可对应多个进程；通过调用关系，一个进程可包括多个程序。

##### 进程的控制原语

* 由若干条指令所组成的指令序列，来实现某个特定的操作功能
  * 指令序列执行是连续的，不可分割 
  * 是操作系统核心组成部分 
  * 必须在管态（内核态）下执行，且常驻内存
* 与系统调用的区别
  * 不可中断

##### 进程的三种基本状态

* 就绪状态：进程已获得除处理机外的所需资源 ，等待分配处理机资源；只要分配CPU就可执行。
* 执行状态：占用处理机资源；处于此状态的进程的数目小于等于CPU的数目。在没有其他进程可以执行时（如所有进程都在阻塞状态）， 通常会自动执行系统的idle进程（相当于空操作 ）。
* 阻塞状态：正在执行的进程，由于发生某种事件而暂时无法执行，便放弃处理机处于暂停状态。![微信截图_20240603132716](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240603132716.png)

##### 进程控制块

* 系统为每个进程定义了一个数据结构：进程控制块PCB（Process Control Block）。 
* 作用： 
  * 进程创建、撤消； 
  * 进程唯一标志；
* 进程控制块是进程管理和控制的最重要的数据结构，每一个进程均有一个PCB，在创建进程时， 建立PCB，伴随进程运行的全过程，直到进程撤消而撤消。

* 进程控制块的内容
  * 进程标识符 
  * 程序和数据地址 
  * 当前状态 
  * 现场保留区 
  * 互斥和同步机制 
  * 进程通信机制 
  * 优先级 
  * 资源清单 
  * 链接字 
  * 家族关系…

##### 线程

* 实际上，进程包含了两个概念 
  * 资源拥有者 
  * 可执行单元
* 现代操作系统将资源拥有者称为进程（process,  task），将可执行单元称为线程（Thread）。
* **引入线程的目的：减小进程切换的开销；提高进程内的并发程度；共享资源**

##### 进程与线程

- 一个进程可以拥有多个线程，而一个线程同时只能被一个进程所拥有。
- 进程是资源分配的基本单位，线程是处理机调度的基本单位，所有的线程共享其所属进程的所有资源与代码。
- 线程共享进程的数据的同时，有自己私有的的堆栈。![微信截图_20240603140722](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240603140722.png)

##### 用户级线程

* 线程在用户空间,通过 library模拟的thread,不需要或仅需要极少的kernel支持 
* 上下文切换比较快,因为不用更改页表等,使用起来 较为轻便快速. 
* 提供操控视窗系统的较好的解决方案.

* 优点
  * 线程切换与内核 无关 
  * 线程的调度由应 用决定，容易进行优化 
  * 可运行在任何操作系统上，只需要线程库的支持
* 缺点：
  * 很多系统调用会引起阻塞，内核会因此而阻塞所有相关的线程。 
  * 内核只能将处理器分配给进程，即使有多个处理器，也无法实现一个进程中的多个线程的并行执行。

##### 内核级线程

* 内核级线程就是kernel有好几个分身,一个分身可以处理一件事. 
* 这用来处理非同步事件 很有用, kernel可以对每个非同步事件产生一个分身来处理. 
* 支持内核线程的操作系统内核称作多线程内核

* 优点
  * 内核可以在多个处理器上调度一个进程的多个线程实现同步并行执行 
  * 阻塞发生在线程级别 
  * 内核中的一些处理可以通过多线程实现
* 缺点：
  * 一个进程中的线程切换需要内核参与，线程的切换涉及到两个模式的切换 （进程-进程、线程-线程） 
  * 降低效率

##### 用户级线程和内核级线程的比较

* 内核级线程是OS内核可感知的，而用户级线程是OS内核不可感知的。 
* 用户级线程的创建、撤消和调度不需要OS内核的支持，是在语言或用户库这一级处理的；而内核级线程的创建、撤消和调度都需OS内核提供支持，而且与进程的创建、撤消和调度大体是相同的。 
* 用户级线程执行系统调用指令时将导致其所属进程的执行被暂停，而内核级线程执行系统调用指令时，只导致该线程被暂停。

* 在只有用户级线程的系统内，CPU调度还是以进程为单位，处于运行状态的进程中的多个线程，由用户程序控制线程的轮换运行；在有内核级线程的系统内，CPU调度则以线程为单位 ，由OS的线程调度程序负责线程的调度。
* 用户级线程的程序实体是运行在用户态下的程序，而内核级线程的程序实体则是可以运行在任何状态下的程序。

#### 同步与互斥

##### 临界资源

* 一次仅允许一个进程访问的资源。

##### 临界区

* 每个进程中访问临界资源的那段代码称为临界区。

##### 进程互斥

* **互斥是指当一个进程进入临界区使用临界资源时，另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。**

* 两个或两个以上的进程，不能同时进入关于同一组共享资源的临界区，否则可能发生与时间有关的错误。

* 间接制约关系
* 访问是无序访问
* 两个或两个以上的进程，不能同时进入关于同一组共享资源的临界区，否则可能发生与时间有关的错误。

##### 进程同步

* 直接制约关系
* 访问是有序访问
* 系统中各进程之间能有效地共享资源和相互合作，从而使程序的执行具有可再现性的过程称为进程同步。
* **同步是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。**

##### 同步互斥原则：

* 空闲让进：临界资源处于空闲状态，允许进程进入临界区。如，临界区内仅有一个进程执行。
* 忙则等待：临界区有正在执行的进程，所有其他进程则不可以进入临界区。
* 有限等待：对要求访问临界区的进程，应在保证在有限时间内进入自己的临界区，避免死等。
* 让权等待：当进程（长时间）不能进入自己的临界区时，应立即释放处理机，尽量避免忙等。

##### 基于忙等待的互斥问题

* 软件方法：

  * Dekker算法：

    ```
    // flag[i]表示第i个进程想要访问临界区的意愿，true表示想要
    // turn = i表示允许第i个进程进入临界区
    P0:
    flag[0]=true;
    while(flag[1]) {
    	if (turn == 1) {
    		flag[0]=false;
    		while(turn == 1);
    		flag[0]=true;
    	}
    }
    --------
    临界区
    --------
    turn = 1;
    flag[0]=false;
    
    P1:
    flag[1]=true;
    while(flag[0]) {
    	if (turn == 0) {
    		flag[1]=false;
    		while(turn == 0);
    		flag[1]=true;
    	}
    }
    --------
    临界区
    --------
    turn = 0;
    flag[1]=false; 
    ```

  * Peterson算法

    ```
    // flag[i]表示第i个进程想要访问临界区的意愿，true表示想要
    // turn = i表示允许第i个进程进入临界区
    P0:
    flag[0]=true;//0进程感兴趣
    turn=1;//让权
    while(flag[1]&&turn==1);
    --------
    临界区
    --------
    flag[0]=false;//不感兴趣
    
    P1:
    flag[1]=true;//1进程感兴趣
    turn=0;//让权
    while(flag[0]&&turn==0);
    --------
    临界区
    --------
    flag[1]=false;//不感兴趣
    ```

  * 面包店算法

    * n 个进程访问临界区
    * 在进入临界区之前，进程收到一个数字，具有 最小数字的进程被允许进入临界区。
    * 如果进程 Pi 和 Pj 接收到相同数字, if i < j, then  Pi 进入临界区；否则，Pj 进入临界区。
    * 产生的数字总是递增的，例如：  1,2,3,3,3,3,4,5...

* 硬件方法

  * 中断屏蔽方法

    * CPU只在发生中断时引起进程切换，因此屏蔽中断能够保证当前运行的进程让临界区代码顺利执行完

      ```
      关中断;
      临界区;
      开中断;
      ```

    * 优点：

      * 简单

    * 缺点：

      * 限制了CPU交替执行程序的能力，系统效率明显降低

  * TestAndSet指令

    * TestAndSet指令是一种不可中断的基本原语
    * 功能是读出指定标志后将该标志设置为真
    * 在多进程可 同时访问内存的情况下，如果一个进程正在执行TS指令， 在它执行完成前，其它的进程不可以执行TS指令。

    ```
    boolean TestAndSet(boolean *lock) {
    	boolean initial = *lock;
    	*lock = true;
    	return initial;
    }
    ```

    * 自旋锁

      * 借助TS指令实现，为每个临界资源设置一个共享布尔变量lock,true表示被占用，false表示空闲，初值为false

      ```
      while(TestAndSet(&lock));
      ------
      临界区
      ------
      lock=false;
      ```

* 软硬件方法共性问题

  * 忙等待: 浪费CPU时间
  * 优先级反转：低优先级进程先进入临界区，高优先级进程一直忙等

##### 信号量

* 信号量只能通过初始化和两个标准的原语来访问，作为OS核心代码执行，不受进程调度的打断。

###### 整型信号量：

* 被定义为一个用于表示资源数目的整型量S。
* 存在忙等
* 只有三个原子操作：
  * 初始化S
  * P操作
  * V操作

* PV操作：

  ```
  P(S) {
  	while(S <= 0);
  	S=S-1;
  }
  ```

  ```
  V(S) {
  	S=S+1;
  }
  ```

###### 记录型信号量

* 不存在忙等
* 需要一个用于代表资源的数目的整型变量value
* 需要一个进程链表L，用于链接所有等待该资源的进程

```
typedef struct {
	int value;
	struct process *L;
} semaphore;
```

* PV操作：

```
P(semaphore S) {//相当于申请资源
	S.value--;
	if (S.value < 0) {
		add this process to S.L;//进程自我阻塞，并插入到该资源的等待队列中
		block(S.L);
	}
}

V(semaphore S) {//相当于释放资源
	S.value++;
	if (S.value <= 0) {
		remove a process C from S.L;//S.value<=0则表明还有进程在等待该资源，因此调用wakeup原语将S.L中第一个进程唤醒。
		wakeup(C);
	}
}
```

##### 利用信号量实现互斥

* 可以设置一个初值为1的信号量S实现进程间的互斥

```
semaphore S = 1;
P1() {
	...
	P(S);
	-----
	临界区
	-----
	V(S);
	...
}
P2() {
	...
	P(S);
	-----
	临界区
	-----
	V(S);
	...
}
```

##### 利用信号量实现同步

* 设置一个信号量S，初值为0

```
semaphore S = 0;
P1() {
	x;//执行语句x
	V(S);//告诉进程P2，语句x已完成
	...
}
P2() {
	...
	P(S);//检查语句x是否运行完成
	y;//获得x的运行结果，执行语句y
	...
}
```

##### PV操作优缺点

* 优点
  * 简单，而且表达能力强
  * 用P.V操作可解决任何同步互斥问题
* 缺点
  * 不够安全
  * P.V操作使用不当会出现死锁
  * 遇到复杂同步互斥问题时实现复杂

##### 管程

* 定义：把分散的临界区集中起来，为每个可共享资源设计一个专门机构来统一管理各进程对该资源的访问，这个专门机构称为管程。

* 管程是在程序设计语言当中引入的一种高级同步原语。
* 管程是一种语言概念，由编译器负责实现互斥
* **任一时刻，管程中只能有一个活跃进程。**
* 管道：单独构成一种独立的文件系统，**并且只存在在内存中**。

##### 条件变量

* 阻塞原因定义为条件变量
* 每个条件变量保存了一个等待队列，用于保存因该条件变量而阻塞的所有进程
* 条件变量上 能作wait和signal原语操作
* 若条件变量名为X，则调用同步原语的形式为X.wait和X.signal。

##### 条件变量与信号量的区别

* 条件变量的值不可增减，P-V操作的信号量值可增减
  * wait操作一定会阻塞当前进程；但P操作只有当信号量的值小于0时才会阻塞。
  * 如果没有等待的进程，signal将丢失；而V操作增加了信号量的值，不会丢失。
  * 访问条件变量必须拥有管程的锁

##### 进程间通信

* IPC：
  * 管道（Pipe）及命名管道（Named pipe或FIFO） 
  * 消息队列（Message） 
  * 共享内存（Shared memory） 
  * 信号量（Semaphore） 
  * 套接字（Socket） 
  * 信号（Signal）

* 低级通信：只能传递状态和整数值（控制信息） ，**包括进程互斥和同步所采用的信号量和管程机制。**
  * 缺点：
    * 传送信息量小：效率低，每次通信传递的信息量固定，若传递较多信息则需要进行多次通信。
    * 编程复杂：用户直接实现通信的细节，编程复杂， 容易出错。
    * 相同地址空间：属于同一个进程的多个线程/共享 地址空间的多个进程
* 高级通信：适用于分布式系统，基于共享内存的多处理机系统，单处理机系统，能够传送任意数量的数据，可以解决进程的同步问题和通信问题
  * 主要包括三类：
    * 管道
    * 共享内存
    * 消息系统

##### 管道

* 管道是半双工的，数据只能向一个方向流动；需要双方通信时，需要建立起两个管道
* 单独构成一种独立的文件系统：管道对于管道两端的进程而言，就是一个文件，但它不是普通的文件，它不属于某种文件系统，而是单独构成一种文件系统，并且**只存在于内存中**
* 数据的读出和写入：一个进程向管道中写的内容被管道 另一端的进程读出。写入的内容每次都添加在管道缓冲区的末尾，并且每次都是从缓冲区的头部读出数据（先进先出）

* **无名管道**
  * 只能用于父子进程或者兄弟进程之间（具有亲缘关系的进程）
* **有名管道**
  * 不相关的进程也能交换数据。
  * 严格遵循先进先出

##### 消息传递

* 两个通信原语（系统调用）
* 调用方式
  * 阻塞调用 
  * 非阻塞调用

##### 共享内存

* 共享内存是最有用的进程间通信方式，也是最快的IPC 形式（因为它避免了其它形式的IPC必须执行的开销巨大的缓冲复制）
* 两个不同进程A、B共享内存的意义是：**同一块物理内存被映射到进程A、B各自的进程地址空间。**
* 当多个进程共享同一块内存区域，由于**共享内存可以同时读但不能同时写**，则需要同步机制约束（互斥锁和信号量都可以）
* 共享内存通信的效率高（因为进程可以直接读写内存）
* 进程之间在共享内存时，保持共享区域直到通信完毕。

##### 经典进程同步问题

###### 生产者－消费者问题

> 问题描述：若干进程通过有限的共享缓冲区交换数据。其中，“生产者”进程不断写入，而“消 费者”进程不断读出；共享缓冲区共有N个；任何时刻只能有一个进程可对共享缓冲区进行操作。

问题分析：

* 关系分析：
  * 生产者和消费者对缓存区互斥访问是互斥关系
  * 生产者和消费者又是同步关系，只有生产者生产后，消费者才能消费
* 信号量设置：
  * 信号量mutex作为互斥信号量，用于控制互斥访问缓存池
  * 信号量full用于记录当前缓存池中”满“缓存区数
  * 信号量empty用于记录”空“缓存区数量

```
semaphore mutex = 1;
semaphore empty = N;
semaphore full = 0;

producer() {
	while(1) {
		P(empty);
		P(mutex);
		produce();
		V(mutex);
		V(full);
	}
}

consumer() {
	while(1) {
		P(full);
		P(mutex);
        consume();
		V(mutex);
		V(empty);
	}
}
```

###### 读者-写者问题

> * 允许多个读者同时对文件执行读操作
> * 只允许一个写者往文件中写信息
> * 任意写者完成写操作前不允许其他读者或写者工作
> * 写者执行写操作前，应让已有的写者和读者全部退出

```
int count = 0;
semaphore mutex = 1;
semaphore rw = 1;//判断缓存区是否为空
semaphore w = 1;//旋转门，防止写者饥饿

writer() {
	while(1) {
		P(w);
		P(rw);
		write();
		V(rw);
		V(w);
	}
}

reader() {
	while(1) {
		P(w);
		P(mutex);
		if (count == 0) {
			P(rw);
		}
		count++;
		V(mutex);
		V(w);
		read();
		P(mutex);
		count--;
		if (count == 0) {
			V(rw);
		}
		V(mutex);
	}
}
```

```
//写者优先
semaphore noReader = 1;
semaphore noWriter = 1;
semaphore mutex = 1;
int countWriter = 0;
int countReader = 0;
Reader() {
	P(noReader);
	P(mutex);
	if (countWriter == 0) {
		P(noWriter);
	}
	countWriter++;
	V(mutex);
	V(noReader);
	read();
	P(mutex);
	countWriter--;
	if (countWriter == 0) {
		V(noWriter);
	}
	V(mutex);
}

Writer() {
	P(mutex);
	if (countReader == 0) {
		P(noReader);
	}
	countReader++;
	V(mutex);
	P(noWriter);
	write();
	V(noWriter);
	P(mutex);
	countReader--;
	if (countReader == 0) {
		V(noReader);
	}
	V(mutex);
}
```



###### 哲学家就餐问题



#### CPU调度

* CPU 调度的任务是控制、协调多个进程对CPU的竞争。也就是按照一定的策略（调度算法），从就绪队列中选择一个进程，并把CPU 的控制权交给所选中的进程。

##### 调度类型

* 高级调度(作业调度)：作业调度，时间上通常是分钟、小时或天
* 中级调度(内存调度)：内外存交换，将进程的部分或全部换出到外存上，将当前所需部分换入到内存。
* 低级调度(进程调度)：进程或线程调度。(抢占/非抢占)

##### 调度的性能准则

* 周转时间：作业从提交到完成（得到结果）所经历的时间。
  * 平均周转时间
  * 带权平均周转时间:周转时间/实际服务时间
* 响应时间：用户输入一个请求（如击键）到系统给出首次响应（如屏幕显示）的时间
* 截止时间：开始截止时间和完成截止时间，与周转时间有些相似。
* 吞吐量：单位时间内所完成的作业数，跟作业本身特性和调度算法都有关系
  * **平均周转时间不是吞吐量的倒数**，因为并发执行 的作业在时间上可以重叠。

##### 批处理系统中常用的调度算法

###### 先来先服务(FCFS)

* 既可用于作业调度，又可用于进程调度
* 最简单的调度算法，按先后顺序调度。
  * 按照作业提交或进程变为就绪状态的先后次序， 分派CPU；
  * 当前作业或进程占用CPU，直到执行完或阻塞， 才让出CPU（非抢占方式）。
  * 在作业或进程唤醒后（如I/O完成），并不立即恢复执行，通常等到当前作业或进程让出CPU。
* 特点：
  * 比较有利于长作业，而不利于短作业。 
  * 有利于CPU繁忙的作业，不利于I/O繁忙的作业。

###### 短作业优先(SJF)

* 是对FCFS算法的改进，其目标是 减少平均周转时间。
* 对预计执行时间短的作业（进程）优先分派处理机。通常后来的短作业不抢占正在执行的作业。
* 优点：
  * 比FCFS改善平均周转时间和平均带权周转时间 ，缩短作业的等待时间； 
  * 提高系统的吞吐量；
* 缺点：
  * 对长作业非常不利，可能长时间得不到执行； 
  * 未能依据作业的紧迫程度来划分执行的优先级
  * 难以准确估计作业（进程）的执行时间，从而影响调度性能。

###### 最短剩余时间优先(SRTN)

* 将短作业优先进行改进，改进为抢占式，就得到最短剩余时间优先算法
* 一个新就绪的进程比当前运行进程具有更短的完成时间，系统抢占当前进程，选择新就绪的进程执行。
* 缺点：源源不断的短任务到来，可能使长的任务长时间得不到运行，导致产生“饥饿”现象。

###### 最高响应比优先(HRRN)

* HRRN算法实际上是FCFS算法和SJF算法的折衷既考 虑作业等待时间，又考虑作业的运行时间，既照顾 短作业又不使长作业的等待时间过长，改善了调度性能
* 在每次选择作业投入运行时，先计算后备作业队列中每个作业的响应比RP，然后选择其值最大的作业投入运行。
*  RP＝(作业已等待时间+作业的服务时间)/作业的服务时间
* 特点：
  * 非抢占式 
  * 短作业容易得到较高的响应比 
  * 长作业等待时间足够长后，也将获得足够高的响应比 
  * 饥饿现象不会发生
* 缺点：
  * 每次计算各道作业的响应比会有一定时间开销。

##### 交互式系统的调度算法

###### 时间片轮转算法(RR)

* 将系统中所有的就绪进程按照FCFS原则，排成 一个队列。
* 每次调度时将CPU分派给队首进程，让其执行一个时间片。时间片的长度从几个ms到几百ms。 
* 在一个时间片结束时，发生时钟中断。 
* 调度程序据此暂停当前进程的执行，将其送到就绪队列的末尾，并通过上下文切换执行当前的队首进程。 
* **进程可以未使用完一个时间片，就让出CPU**（如阻塞）。
* 时间片长度变化的影响
  * 过长－>退化为FCFS算法，进程在一个时间片内都执行完，响应时间长。
  * 过短－>用户的一次请求需要多个时间片才能处理完，上下文切换次数增加，响应时间长。

###### 优先级算法

* 既可用于作业调度，又可用于进程调度
* 静态优先级
  * 创建进程时就确定，直到进程终止前都不改变 。通常是一个整数。
* 动态优先级
  * 在创建进程时赋予的优先级，在进程运行过程中可以自动改变，以便获得更好的调度性能。

###### 多级队列调度算法

* 本算法引入多个就绪队列，通过各队列的区别对待，达到综合的调度目标
* 不同队列可有不同的优先级、时间片长度、调度策略等；在运行过程中还可改变进程所在队列。如：系统进程、用户交互进程、批处理进程等。

###### 多级反馈队列调度算法

* 多级反馈队列算法是时间片轮转算法和优先级算法的综合和发展。
* 设置多个就绪队列，分别赋予不同的优先级（如逐级降低），队列1的优先级最高。每个队列执行时间片的长度也不同，规定优先级越低则时间片越长（如逐 级加倍）。
* 新进程进入内存后，先投入队列1的末尾，按FCFS算法调度；若按队列1一个时间片未能执行完，则降低投入到队列2的末尾，同样按FCFS算法调度；如此下 去，降低到最后的队列，则按“时间片轮转”算法调度直到完成。
* 仅当较高优先级的队列为空，才调度较低优先级的队列中的进程执行。**如果进程执行时有新进程进入较高优先级的队列，则抢先执行新进程，并把被抢先的进 程投入原队列的末尾。**

*  I/O型进程：让其进入最高优先级队列，以及时响应I/O交互。通常执行一个小时间片，要求可处理完一次I/O请求的数据，然后转入到阻塞队列。 
* 计算型进程：每次都执行完时间片，进入更低级队列。最终采用最大时间片来执行，减少调度次数。 
* I/O次数不多，而主要是CPU处理的进程：在I/O 完成后，放回I/O请求时离开的队列，以免每次都回到最高优先级队列后再逐次下降。 
* 为适应一个进程在不同时间段的运行特点，I/O完成时，提高优先级；时间片用完时，降低优先级；

##### 优先级倒置现象

* 高优先级进程（或线程）被低优先级进程（或线程）延迟或阻塞。
  * 例如：有三个完全独立的进程Task A、Task B 和 TaskC，Task A 的优先级最高，Task B 次之，Task  C 最低。Task A 和Task C 共享同一个临界资源X
  * Task A 和Task C 共享同 一个临界资源，高优先级 进程Task A因低优先级进 程Task C被阻塞，又因为 低优先级进程Task B的存 在延长了被阻塞的时间 （因为B的优先级高于C的优先级）。

* 解决方法——优先级置顶：
  * 进程Task C 在进入临界区后，Task C 所占用的处理机就不允许被抢占。这种情况下，Task C 具有最高优先级（Priority Ceiling）。
  * 如果系统中的临界区都较短且不多，该方法是可行的。反之，如果Task C  临界区非常长，则高优先级进程Task A 仍会等待很 长的时间，其效果无法令 人满意。
* 解决方法——优先级继承：
  * 当高优先级进程Task A 要进入临界区使用临界资源X时，如果已经有一个低优先级进程Task C 正在使用该资源，可以采用优先级继（Priority  Inheritance）的方法。
  * 此时一方面Task A 被阻塞，另一方面由Task C继承Task A的优先级， 并一直保持到Task C 退出临界区。

##### 实时系统的调度算法

* 前提条件
  * 任务集（S）是已知的； 
  * 所有任务都是周期性（T）的，必须在限定的时限D内 完成； 
  * 任务之间都是独立的，每个任务不依赖于其他任务； 
  * 每个任务的运行时间（c）是不变的； 
  * 调度, 任务切换的时间忽略不计。

###### 静态表调度算法(Static table-driven scheduling)

* 通过对所有周期性任务的分析预测（到达时间 、运行时间、结束时间、任务间的优先关系） ，事先确定一个固定的调度方案。
* 特点：
  * 无任何计算，按固定方案进行，开销最小； 
  * 无灵活性，只适用于完全固定的任务场景。

###### 单调速率调度(RMS)

* RMS是单处理器下的最优静态调度算法。1973 年Liu和Layland首次提出了RM调度算法，并证 明了其在静态调度中的最优性. 它的一个特点是 可通过对系统资源利用率的计算来进行任务可调度性分析, 算法简单、有效, 便于实现。
* 特点:
  * 任务的周期越小，其优先级越高，优先级最高的任 务最先被调度 
  * 如果两个任务的优先级一样，当调度它们时，RM 算法将随机选择一个调度
* 静态、抢占式调度

###### 最早截止期优先(EDF)

* 任务的绝对截止时间越早，其优先级越高，优先级最高的任务最先被调度
* 如果两个任务的优先级一样，当调度它们时， EDF算法将随机选择一个调度
* 任务集可调度的充分必要条件: 求和(Ci / Ti)(i=1,2,3...n) <= 1;

###### 最低松弛度优先算法(LLF)

* LLF算法是根据任务紧急（或松弛）的程度，来 确定任务的优先级。任务的紧急度越高，其优先 级越高，并使之优先执行。
* 松弛度（Laxity） = 任务截止时间- 本身剩余运行时间- 当前时间
* 调度时机：有进程执行完或有进程的Laxity为0时 （抢占）。
* 任务集可调度,如果：求和(Ci / Ti)(i=1,2,3...n) <= 1

#### 死锁

* 死锁：一组进程中，每个进程都无限等待被该组进程中其它进程所占有的资源，在无外力介入的条件下，将因永远分配不到资源而无法运行的现象。

##### 死锁发生的四个必要条件

* 互斥条件：指进程对所分配到的资源进行排它性使用， 即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。
* 请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。
* 不可剥夺条件：指进程已获得的资源，在未使用完之前 ，不能被剥夺，只能在使用完时由自己释放。
* 环路等待条件：指在发生死锁时，必然存在一个进程— —资源的环形链，即进程集合{P0，P1，P2，···， Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2 占用的资源，……，Pn正在等待已被P0占用的资源。

##### 活锁与饥饿

* 活锁：是指任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。
  * 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，即所谓的“活”，而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。 避免活锁的简单方法是采用先来先服务的策略。
* 饥饿：某些进程可能由于资源分配策略的不公平导致长时间等待。当等待时间给进程推进和响应带来明显影响时，称发生了进程饥饿，当饥饿到一定程度的进程所赋予的任务即使完成也不再具有实际意义时称该进程被饿死。

##### 处理死锁的基本方法

###### 不允许死锁发生

* 死锁预防：

  * **死锁预防是排除死锁的静态策略，它使产生死锁的四个必要条件不能同时具备**，从而对进程申请资源的活动加以限制，以保证死锁不会发生。

  * 打破互斥条件：即允许进程同时访问某些资源 。但是，有的资源是不允许被同时访问的，像 打印机等等，这是资源本身的属性。
  * 打破请求且保持的条件：可以实行资源预先分配策略。只有当系统能够满足当前进程的全部资源需求时，才一次性地将所申请的资源全部分配给该进程，否则不分配任何资源。由于运行的进程已占有了它所需的全部资源，所以不会发生占有资源又申请资源的现象，因此不会发生死锁。
    * 缺点：
      * 在许多情况下，由于进程在执行时是动态的， 不可预测的，因此不可能知道它所需要的全部 资源。
      * 资源利用率低。无论资源何时用到，一个进程 只有在占有所需的全部资源后才能执行。即使 有些资源最后才被用到一次，但该进程在生存 期间却一直占有。这显然是一种极大的资源浪 费；
      * 降低进程的并发性。因为资源有限，又加上存 在浪费，能分配到所需全部资源的进程个数就 必然少了。
  * 打破不可剥夺条件：即允许进程强行从占有者那里夺取某些资源。
    * 当一个进程已占有了某些资源，它又申请新的资源，当不能立即被满足时，须释放所占有的全部资源，以后再重新申请。 
    * 它所释放的资源可以分配给其它进程，相当于该进程占有的资源被隐蔽地强占了。 
    * 这种预防死锁的方法实现起来困难，会降低系统性能。
  * 打破循环等待条件：实行资源有序分配策略。即把资源事先分类编号，按号分配，使进程在申请 ，占用资源时不会形成环路。所有进程对资源的请求必须严格按资源序号递增的顺序提出。进程占用了小号资源，才能申请大号资源，就不会产生环路，从而预防了死锁。这种策略与前面的策略相比，资源的利用率和系统吞吐量都有很大提高，但存在以下缺点：
    * 限制了进程对资源的请求，同时给系统中所有资源 合理编号也是件困难事，并增加了系统开销；
    * 为了遵循按编号申请的次序，暂不使用的资源也需要提前申请，从而增加了进程对资源的占用时间。

* 死锁避免

  * **死锁的避免是排除死锁的动态策略，它不限制进程有关资源的申请，而是对进程所发出的每一个申请资源命令加以动态地检查，并根据检查结果决定是否进行资源分配**。即分配资源时判断是否会出现死锁，有则加以避免。如不会死锁，则分配资源。

  * 安全序列的定义：一个序列{P1，P2，...，Pn} 安全的，是指若对于每一个进程Pi，它需要的资源可以被系统中当前可用资源加上所有进程 Pj（j < i）当前占有资源之和所满足，则{P1， P2，...，Pn}为一个安全序列。
  * 如果存在安全序列，则处于安全状态，不存在安全序列的系统是不安全的。
  * 安全状态：系统存在一个进程执行序列可顺利完成
  * **系统处于安全状态一定不会发生死锁，系统进入不安全状态（四个死锁的必要条件同时发生）未必会产生死锁。产生死锁后， 系统一定处于不安全状态。**

  * **银行家算法**

###### 允许死锁发生

*  检测与解除死锁
  * 死锁检测：资源分配图
    * 有向图G的顶点为资源或进程，从资源 R到进程P的边表示R已分配给P，从进程P到资源R的边表示P正因请求R而处于等待状态。
    * **如果一个资源分配图中存在环路， 不一定存在死锁**
    * 当Pi有请求边时，首先将其请求边变成分配边(即满足Pi的资源请求)，而一旦Pi的所有资源请求都得到满足，Pi就能在有限的时间内运行结束，并释放其所占用的全部资源，此时Pi只有分配边， 删去这些分配边（实际上相当于消去了Pi的所有请求边和分配边），使Pi成为孤立结点。（反复进行）
    * 死锁定理：系统中某个时刻t为死锁状态的充要条件是t时刻系统的资源分配图是不可完全化简的。
    * 在经过一系列的简化后，若能消去图中的所有边，使所有的进程都成为孤立结点，则称该图是可完全化简的；反之的是不可完全化简的。
  * 死锁解除：
    * 剥夺资源：使用挂起/激活挂起一些进程，剥夺它们的资源以解除死锁，待条件满足时，再激活进程。
    * 撤消进程：使全部死锁的进程夭折掉；按照某种顺序逐个地撤消（回退）进程，直至有足够的资源可用，死锁状态消除为止

### 第5章

#### 设备管理

##### 设备的管理的目的和功能

* 外设管理目的 
  * 提高效率：提高I/O访问效率，匹配CPU和多种不同处理速 度的外设 •
  * 方便使用：方便用户使用，对不同类型的设备统一使用方法 ，协调对设备的并发使用 
  * 方便控制：方便OS内部对设备的控制：增加和删除设备，适应新的设备类型

* 外设管理功能 
  * 提供设备使用的用户接口：命令接口和编程接口。
  * 设备分配和释放：使用设备前，需要分配设备和相应的通道 、控制器。  
  * 设备的访问和控制：包括并发访问和差错处理。  
  * I/O缓冲和调度：目标是提高I/O访问效率。

##### I/O管理示意（软件角度）

* **总线(Bus)：接入I/O设备的主要方式**
  * 数据线
  * 控制线
  * 地址线
* 控制器中包含控制寄存器、状态寄存器，以及一些数据寄存器。（I/O端口）
* 在OS中，I/O 设备管理可直接从应用程序或文件系统得 到请求，并负责完成这个请求。它具体包括：
  * 逻辑I/O:完成设备无关的操作，如设备分配，设备回收， 数据准备等；
  * 设备驱动程序：负责对设备控制器进行控制（通过读写其中的寄存器）。
  * 中断服务程序：设备工作结束后负责向CPU发中断信号, 中断服务程序完成相应处理。

##### I/O设备的分类

* 字符设备：
  * 字符设备是以字符或字节为单位进行数据传输的设备，数据通常以连续的序列形式处理。**这类设备不支持随机访问，数据读写操作是顺序进行的，即不可寻址。**
  * 它们通常没有缓冲区，操作直接作用于设备，**适用于实时数据传输**，如键盘、鼠标、串行端口、打印机和终端等。
  * 字符设备的特点包括顺序访问、非缓存操作，适合于文本数据或少量数据的快速传输。 
* 块设备：
  - 块设备以固定大小的数据块为单位进行读写操作，每个块都可以独立寻址，支持随机访问。
  - 这类设备通常使用缓冲区来提高效率，数据先被读取到缓冲区或从缓冲区写入，然后再进行实际的I/O操作，适合于磁盘驱动器、SSD、CD/DVD驱动器、闪存驱动器等存储设备。
  - 块设备的读写操作不是立即发生的，可能涉及缓存管理和优化策略，以减少访问延迟并提高吞吐量。
* 网络设备：
  - 网络设备是指用于网络通信的硬件，如网卡、路由器、交换机等，它们负责数据包在网络中的传输。
  - 这类设备的工作原理不同于字符和块设备，它们处理的是网络数据包，遵循网络协议栈，如TCP/IP，进行数据的封装、解封装和路由。
  - 网络设备在操作系统中通常通过网络子系统和相应的设备驱动程序进行管理，支持计算机之间的数据交换。
* 独占设备：在一段时间内只能由一个进程使用的 设备。通常独占设备的传输速率比较慢，打印机 和磁带机都属于典型的独占设备。 
* 共享设备：在一段时间内允许多个进程共同使用 的设备。多个进程以交叉的方式来使用设备。资 源利用率较高。硬盘是典型的共享设备。 计算机学院 
* 虚设备：在一类设备上模拟另一类设备，常用的 方法是，用共享设备模拟独占设备，用高速设备 模拟低速设备。如：用Spooling技术将打印机变成 共享设备

#### I/O硬件组成

##### 设备控制器

* 控制器的功能 
  * 接收和识别CPU命令 
  * 数据交换：CPU与控制器、控制器与设备 
  * 设备状态的了解和报告 
  * 设备地址识别 
  * 缓冲区 
  * 对设备传来的数据进行差错检测
* 组成 
  * 控制器与CPU接口：数据寄存器、控制寄存器、状态寄存器，采用内存映射或专门的I/O指令 
  * 控制器与设备接口：数据信号、控制信号、状态信号 
  * I/O逻辑：用于实现CPU对I/O设备的控制

##### I/O端口地址

* I/O端口地址：接口电路中每个寄存器具有唯一的地址
*  I/O指令形式与I/O地址是相互关联的，主要有以下形式： 
  * 内存映射I/O
    * 控制器的内存/寄存器作为物理内存空间的一部分
    * 优点：
      * 不需要特殊的保护机制来阻止用户进程进行相应的 I/O 操作。操作系统要避免把包含了控制寄存器的那部分地址空间放入用户的虚拟地址空间中。 
      * 可以引用内存的每一条指令都可以适用于引用控制寄存器。
    * 缺点：
      * 不允许对一个控制寄存器的内容进行高速缓存。
      * 使主存容量变小
      * 使译码电路变复杂，降低寻址速度
  * I/O独立编址
    * 端口地址与主存空间是两个独立的地址空间
    * 优点：
      * 外设不占用内存的地址空间 
      * 编程时，易于区分是对内存操作还是对I/O 操作
      * 寻址速度快
    * 缺点：
      * I/O端口操作的指令类型少，操作不灵活

#### I/O控制方式

##### 程序控制I/O

* 也称**轮询**或查询方式I/O，它由CPU代表进程向I/O模块发出指令，然后进入忙等状态，直到操作完成之后进程才能够继续执行。
  - 优点：实现简单
  - 缺点：CPU效率低

##### 中断驱动

* 当I/O操作结束后由设备控制器主动地来通知设备驱动程序，而不是依靠设备驱动程序不断地去轮询看看设备的状态。  
  - 优点：提高了CPU利用率，可以处理不确定事件。
  - 缺点：每次输入/输出一个数据都要中断CPU，多次中断浪费CPU时间，只适于数据传输率较低的设备。

##### DMA

* 直接存储器访问方式，是由一个专门的控制器来完成数据从内存到设备或者是从设备到内存的传输工作 。

* CPU要为其设置内存起址，传送字节数

* 基本传送单位是数据块，不再是字

* 所传送数据直接从设备到内存，不在经过CPU

* 仅在传送一个或多个数据块的开始和结束时，才需要CPU干预。

  - 优点：CPU只需干预I/O操作的开始和结束，而其中的一批数据读写无需CPU控制，适于高速设备(数据块传输)。

  - 缺点：

    * 数据传送的方向、存放数据的内存地址及传送数据的长度等都由CPU控制，占用了CPU时间。

    * 每个设备占用一个DMA控制器，当设备增加时，需要增加新的DMA控制器。（一对一）

##### 通道

* 通道是一个特殊功能的处理器，它有自己的指令和程序专门负责数据输入输出的传输控制。
* 通道与CPU共享内存，实现了CPU内部运算与I/O设备的并行工作。
* CPU只需要向通道发送一条I/O指令，通道收到指令完成任务后向CPU发送中断请求
  - 优点：执行一个通道程序可以完成几组I/O操作，与DMA相比，减少了CPU干预。一个通道可以控制多台设备与内存的数据交换。（一对多）
  - 缺点：费用较高

#### I/O软件设计

##### 用户层软件

* 用户层I/O软件通过系统调用获取操作系统的服务

##### 设备无关软件

* 命名、保护、阻塞、缓冲、分配设备
* **设备独立性**：
  * 为了实现设备独立性而引入了**逻辑设备和物理设备**这两个概念，在应用程序中，使用逻辑设备名称来请求使用某类设备，而系统在实际执行时，还必须使用物理设备名称。
  * 系统需具有将逻辑设备名称转换为某物理设备名称的功能
  * 优点：
    * 设备分配时的灵活性
    * 易于实现I/O重定向，用于I/O操作的设备可以更换，而不必改变应用程序。
  * 逻辑设备表：为了实现设备的独立性，系统必须设置一张逻辑设备表LUT(Logical Unit Table)，用于将应用程序中所使用的逻辑设备名映射为物理设备名。
  * 该表的每个表目中包含了三项，逻辑设备名、物理设备名、设备驱动程序的入口地址。通过逻辑设备名，系统可以查找LUT，便可找到物理设备和驱动程序。
    * LUT的设置可以采用两种方式：
      * 在整个系统中设置一张LUT（由于系统中所有进程 的设备分配情况都记录在同一张LUT中，因而不允 许在LUT中具有相同的逻辑设备名，这就要求所有 用户都不能使用相同的逻辑设备名，多用户下难 以做到，单用户很好实现）。 
      * 为每个用户设置一张LUT（每当用户登录时，便为该用户建立一个进程，同时建立一张LUT，并将该表放入进程的PCB中）。
* 设备驱动程序
  * 与设备密切相关的代码放在设备驱动程序中， **每个设备驱动程序处理一种设备类型。**
  * 设备驱动程序的任务是接收来自与设备无关的上层软件的抽象请求，并执行这个请求。
  * 每一个控制器都设有一个或多个设备寄存器， 用来存放向设备发送的命令和参数。设备驱动程序负责释放这些命令，并监督它们正确执行
  * 特点：
    * I/O进程与设备控制器之间的通信程序 
    * 驱动程序与I/O设备的特性紧密相关 
    * 与I/O控制方式紧密相关
    * 与硬件紧密相关

#### 缓冲技术

* 缓冲技术可提高外设利用率。
* 原因 
  * 匹配CPU与外设的不同处理速度 
  * 减少对CPU的中断次数。 
  * 提高CPU和I/O设备之间的并行性。

##### 单缓冲

* 每当用户进程发出一个I/O请求时，操作系统便在主存中为之分配一个缓冲区![微信截图_20240604170621](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240604170621.png)

##### 双缓冲

* 两个缓冲区，CPU和外设都可以连续处理而无需等待对方。

![微信截图_20240604170905](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240604170905.png)

##### 环形缓冲

* 若CPU和外设的处理速度差较大，双缓冲效果不够理 想，因此引入了多缓冲机制，可将多个缓冲组织成循环 缓冲形式。对于用作输入的循环缓冲，通常是提供给输 入进程或计算进程使用，输入进程不断向空缓冲去输入 数据，而计算进程则从中提取数据进行计算。
* 循环缓冲区的组成如下 
  * 多个缓冲区，在循环缓冲区中包括多个缓冲区，每个缓冲区的大小相同，作为输入的多缓冲区可分为三种类型，用于装输入数据的空缓冲区R、已装满数据的缓冲区G以及计算进程正在使用的工作缓冲区C。 
  * 多个指针，作为输入的缓冲区可设置三个指针，用于指示计算进程下一个可用缓冲区G的指针Nextg、指示输入进程下次可用的空缓冲区R的指针Nexti、以及用于指示计算进程正在使用的缓冲区C的指针Current

##### 缓冲池

* 在池中设置了多个可供若干个进程共享的缓冲区。

![微信截图_20240604172046](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240604172046.png)

#### SPOOLing技术

* 也称为虚拟设备技术可把独享设备转变成 具有共享特征的虚拟设备，从而提高设备利用率
* 在多道程序系统中，专门利用一道程序（ SPOOLing程序）来完成对设备的I/O操作。无需使用外围I/O处理机。
*  SPOOLing程序和外设进行数据交换：实际 I/O  
  * SPOOLing程序预先从外设读取数据并加以缓冲， 在以后需要的时候输入到应用程序； 
  *  SPOOLing程序接受应用程序的输出数据并加以缓冲，在以后适当的时候输出到外设

* SPOOLing系统由三部分组成：
  * 输入井和输出井，这是在磁盘上开辟的两个大存 储空间。输入井是模拟脱机输入时的磁盘设备， 用于暂存I/O设备输入的数据，输出井是模拟脱机输出时的磁盘，用于暂存用户程序和输出数据。 
  * 输入缓冲区和输出缓冲区，缓和CPU与磁盘之间速度不匹配，在内存中开辟的两个缓冲区，输入缓冲区用于暂存由输入设备送来的数据，以后再传送到输入井，输出缓冲区用于暂存从输出井送来的数据，以后再传送给输出设备
  * 输入进程SPi和输出进程SPo，利用两个进程来模 拟脱机I/O时的外围控制机，其中，进程SPi模拟 脱机输入时的外围控制机，将用户要求的数据从 输入机通过输入缓冲区再送到输入井，当CPU需 要输入数据时，直接从输入井读入内存；进程 SPo模拟脱机输出时的外围控制机，把用户要求 输出的数据先从内存送到输出井，待输出设备空 闲时，再将输出井中的数据经过输出缓冲区送到 输出设备上
* 特点 
  * 高速虚拟I/O操作：应用程序的虚拟I/O比实 际I/O速度提高，缩短应用程序的执行时间（ 尽快完成计算，并释放占用的计算机资源） 。另一方面，程序的虚拟I/O操作时间和实际 I/O操作时间分离开来。
  * 实现对独享设备的共享：由SPOOLing程序提供虚拟设备，可以对独享设备依次共享使用

### 磁盘存储管理

* 磁盘访问时间 = 寻道时间 + 旋转延迟时间 + 传输时间

* 寻道时间T = m * n + s, m是与磁盘驱动器速度有关的常数，n是跨越n条磁道的时间，s是启动磁头臂的时间。
* 旋转延迟时间Tr。磁头定位到要读写的扇区的时间，设磁盘的旋转速度为r，则Tr = 1 / (2*r). RPM是指多少转每分钟，每转就需60/RPM秒，平均旋转延迟时间就等于30/RPM秒
* 传输时间。从磁盘读出或向磁盘写入数据所需时间，这个时间取决于每次所读写的字节数b和磁盘的旋转速度，则Tt=b/(r*N)。r是磁盘每秒的转数，N为一个磁道上的字节数。

#### 磁盘调度算法

##### 先来先服务算法（FCFS）

* 按访问请求到达的先后次序服务。
* 优点：
  * 简单，公平。 
* 缺点： 效率不高，相邻两次请求可能会造成最内到最外的柱面寻道，使磁头反复移动，增加了服务时间，对机械部件也不利

##### 最短寻道时间优先算法（SSTF）

* 优先选择距当前磁头最近的访问请求进行服务，主要考虑寻道优先。
* 优点：
  * 改善了磁盘平均服务时间。
* 缺点： 可能产生“饥饿” 现象，造成某些访问请求长期等待得不到服务。

##### 扫描算法（SCAN）电梯调度

* 当有访问请求时，磁头按一个方向移动，在移动过程 中对遇到的访问请求进行服务，直到磁头步进到最内 （最外）侧磁道调转扫描方向。
* 优点： 克服了最短寻道优先的缺点，既考虑了距离，同时又 考虑了方向
* 缺点： 但由于是摆动式的扫描方法，两侧磁道被访问的频率 仍低于中间磁道。

##### 循环扫描算法（C-SCAN）

* 移动臂到达最后一个柱面后，立即带动读写磁头快速返回到0号柱面。

##### LOOK

* 磁头只需移动到最远端的请求即可

##### CLOOK

* 移动臂到达最远端的请求后，立即带动读写磁头快速返回到另一侧最远端的请求。

#### 提高I/O速度的主要途径

* 选择性能好的磁盘 
* 并行化 
* 采用适当的调度算法 
* 设置磁盘高速缓冲区
* 提前读
* 延迟写

#### RAID

* 基本思想：把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、容量巨大的硬盘。
* 一种把多块独立的硬盘（物理硬盘）按照不同方式组合起来形成一个硬盘组（逻辑硬盘），从而提供比单个硬盘更高的存储性能和提供数据冗余的技术。
* 组成磁盘阵列的不同方式称为RAID级别(RAID Levels)
* 数据冗余的功能是在用户数据一旦发生损坏后，利用 冗余信息可以使损失数据得以恢复，从而保障了用户 数据的安全性。
* 优点：
  * 成本低，功耗小，传输速率高
  * 可提供容错功能
* **数据分段并行交叉存取**
* 主流的RAID有七个 级别，分别是0、1 、2、3、4、5、6， 还有一些组合级别 如RAID0+1等。

##### RAID 0

* 条带化（Stripe）存储；理论上，有N个磁盘组成的 RAID0是单个磁盘读写速度的N倍。 
* 连续以位或字节为单位分割数据，并行读/写多个磁盘 ，因此具有很高的数据传输率，但没有数据冗余。

* 该级仅提供了并行交叉存取。它虽然有效提高了磁盘 I/O速度，但并**无冗余校验功能。**

##### RAID1

* 镜象（Mirror）存储。 
* 通过磁盘数据镜像实现数据冗余， 在成对的独立磁盘上产生互为备份的数据。 
* 当原始数据繁忙时，可直接从镜像拷贝中读取数据，因此RAID 1可以提高读取性能。 
* RAID 1是磁盘阵列中**单位成本最高**的，但提供了很高的数据安全性和可用性。 
* 当一个磁盘失效时，系统可以自动切换到镜像磁盘上读写，而不需要重组失效的数据。
* 读性能好，写性能由最差的磁盘决定； 可靠性高；代价较高。

##### RAID2

* 海明码（Hamming Code）校验+条带存储
* 将数据条块化地分布于不同的硬盘上，条块单位为位或字节，使用海明码来提供错误检查及恢复。这种编码技术需要多个磁盘存放检查及恢复信息。

* 特点：
  * 并行存取，各个驱动器同步工作。 
  * 使用海明编码来进行错误检测和纠正，数据传输率高 。 
  * 需要多个磁盘来存放海明校验码信息，冗余磁盘数量与数据磁盘数量的对数成正比。 
  * 是一种在多磁盘易出错环境中的有效选择，并未被广泛应用，目前还没有商业化产品。

##### RAID 3

* 采用奇偶校验冗余的磁盘阵列，也采用数据位交叉 ，阵列中只有一个校验盘。
* 特点：
  * 将磁盘分组，采用字节级别的条带，读写要访问组中所有盘，每组中有一个盘作为校验盘。 
  * 校验盘一般采用奇偶校验。 
  * 简单理解：先将分布在各个数据盘上的一组数据加起来，将和存放在冗余盘上。一旦某一个盘出错， 只要将冗余盘上的和减去所有正确盘上的数据，得 到的差就是出错的盘上的数据。

* 缺点：
  * 恢复时间长

##### RAID4

* 并行处理磁盘阵列：一种独立传送磁盘阵列，采用**数据块交叉**，用一个校验盘。将 数据按块交叉存储在多个磁盘上。
* 特点：
  * 冗余代价与RAID3相同 §访问数据的方法与RAID3不同 
  * 在RAID3中，一次磁盘访问将对磁盘阵列中的所有磁盘进行（同步）操作。 
  * RAID4出现的原因：希望使用较少的磁盘参与操作，以使磁盘阵列可以并行进行多个数据的磁盘操作。 
  * 随机读快，随机写慢（竞争同一个校验盘）

##### RAID5

* 一种独立传送磁盘阵列，采用数据块交叉和分布的冗余校验，将数据和校验都分布在各个磁盘中，没有专门的奇偶校验驱动器。
* 特点：
  * RAID 5更适合于小数据块和随机读写的数据。 
  * RAID 3与RAID 5相比，最主要的区别在于RAID 3每进 行一次数据传输就需涉及到所有的阵列盘；而对于 RAID 5来说，大部分数据传输只对一块磁盘操作，并 可进行并行操作。 
  * 在RAID 5中有“写损失”，即每一次写操作将产生四个实际的读/写操作，其中两次读旧的数据及奇偶信息 ，两次写新的数据及奇偶信息。 
  * 当进行恢复时，如我们需要恢复图中的A0，这里就必须需要B0、C0、D0加 parity才能计算并得出A0，进行数据恢复。所以当有两块盘坏掉的时候，整个RAID的数据失效。

##### RAID6

* 双维校验独立存取盘阵列，数据以块（块大小可变 ）交叉方式存于各盘，检、纠错信息均匀分布在所 有磁盘上。
* 特点：
  * 写入数据要访问1个数据盘和2个冗余盘； 
  * 可容忍双盘出错； 
  * 存储开销是RAID5的两倍（多一个冗余盘）。

##### RAID小结

* 条带化：一个字节块可能存放在多个数据盘上 
  * 优点：并行存取，性能好，磁盘负载均衡 
  * 缺点：可靠性、不同IO请求需要排队 
* 镜像：数据完全拷贝一份 
  * 优点：可靠性 
  * 缺点：存储开销 
* 校验：数据通过某种运算（异 或）得出，用以检验该组数字的正确性 
  * 优点：可靠性，快速恢复 
  * 缺点：开销![微信截图_20240604193110](%E6%A6%82%E5%BF%B5%E5%A4%8D%E4%B9%A0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240604193110.png)

s